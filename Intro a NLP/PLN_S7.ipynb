{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNofMmWdWtDJTqPlIK6WMqv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Procesamiento de Lenguaje Natural con Python\n","\n","### Profesor: José Incio, Ph.D\n","#### Departamento de Ciencias Sociales - Sociología\n","- correo: jincio@pucp.pe\n","- web: www.joseincio.com"],"metadata":{"id":"A_EpYKzZ4XkL"}},{"cell_type":"markdown","source":["## Clasificación de documentos y Machine Learning para texto."],"metadata":{"id":"c9GEV1ti4sUm"}},{"cell_type":"markdown","source":["**Presentación sobre clasificación de documentos con Machine Learning**\n","\n","**Introducción**\n","\n","La clasificación de documentos es una tarea importante en muchas áreas, como la investigación, el comercio electrónico y la atención al cliente. En el pasado, esta tarea se realizaba manualmente, pero esto puede ser lento y costoso, especialmente para grandes conjuntos de datos.\n","\n","El Machine Learning (ML) ofrece una solución a este problema. Los algoritmos de ML pueden aprender a clasificar documentos de forma automática, basándose en un conjunto de datos de entrenamiento. Una vez entrenado, el algoritmo de ML puede clasificar nuevos documentos con una alta precisión.\n","\n","**Lógica de la clasificación de documentos con ML**\n","\n","Los algoritmos de ML para la clasificación de documentos funcionan de la siguiente manera:\n","\n","1. **Preprocesamiento de datos:** El primer paso es preprocesar los datos de entrenamiento. Esto puede incluir tareas como la eliminación de caracteres especiales, la corrección de errores ortográficos y la conversión de todos los documentos a un formato común.\n","2. **Extracción de características:** Una vez que los datos han sido preprocesados, el algoritmo de ML extrae características de cada documento. Las características son propiedades de los documentos que pueden ayudar al algoritmo a clasificarlos. Por ejemplo, algunas características comunes para la clasificación de documentos de texto son las palabras clave, los temas y la longitud del documento.\n","3. **Entrenamiento del algoritmo:** El siguiente paso es entrenar el algoritmo de ML. Esto se hace alimentando al algoritmo los datos de entrenamiento y dejándolo que aprenda a clasificar los documentos en función de las características extraídas.\n","4. **Clasificación de nuevos documentos:** Una vez que el algoritmo ha sido entrenado, puede ser utilizado para clasificar nuevos documentos. Para ello, el algoritmo simplemente extrae las características del nuevo documento y las compara con las características de los documentos de entrenamiento. El algoritmo clasifica entonces el nuevo documento en la categoría que mejor se ajuste a sus características.\n","\n","**Técnicas más utilizadas para la clasificación de documentos con ML**\n","\n","Existen muchas técnicas diferentes de ML que pueden utilizarse para la clasificación de documentos. Algunas de las técnicas más utilizadas son:\n","\n","* **Máquinas de vectores de soporte (SVM):** Las SVM son un tipo de algoritmo de ML que puede utilizarse para clasificar datos no lineales. Las SVM funcionan creando un hiperplano que separa los datos en dos categorías.\n","* **Árbol de decisión:** Un árbol de decisión es un tipo de algoritmo de ML que puede utilizarse para clasificar datos mediante la creación de un conjunto de reglas. Cada regla en el árbol de decisión comprueba una característica del documento y devuelve una clasificación.\n","* **Naive Bayes:** Naive Bayes es un tipo de algoritmo de ML basado en la teoría de Bayes. Naive Bayes funciona calculando la probabilidad de que un documento pertenezca a una determinada categoría en función de sus características.\n","\n","*\n","**Conclusión**\n","\n","La clasificación de documentos con ML es una técnica poderosa que puede utilizarse para automatizar la clasificación de grandes conjuntos de documentos. Los algoritmos de ML pueden aprender a clasificar documentos con una alta precisión, lo que puede ahorrar tiempo y dinero a las organizaciones."],"metadata":{"id":"LDFUVX575_6e"}},{"cell_type":"markdown","source":["*Aplicaciones de la clasificación de documentos con ML**\n","\n","La clasificación de documentos con ML tiene una amplia gama de aplicaciones, incluyendo:\n","\n","* **Clasificación de correo electrónico:** Los algoritmos de ML pueden utilizarse para clasificar los correos electrónicos en categorías como spam, importante y personal.\n","* **Clasificación de documentos legales:** Los algoritmos de ML pueden utilizarse para clasificar los documentos legales en categorías como contratos, facturas y demandas.\n","* **Clasificación de artículos científicos:** Los algoritmos de ML pueden utilizarse para clasificar los artículos científicos en categorías como temas, autores y revistas.\n","* **Clasificación de noticias:** Los algoritmos de ML pueden utilizarse para clasificar las noticias en categorías como deportes, política y negocios.\n","\n","\n"],"metadata":{"id":"7HcX6eF7v4dR"}},{"cell_type":"markdown","source":["Google Cloud Natural Language API\n","\n","Para utilizar Google Cloud Natural Language API para la clasificación de documentos, siga estos pasos:\n","\n","Cree una cuenta de Google Cloud Platform y habilite la API de Natural Language.\n","Instale la biblioteca de clientes de Natural Language API para su lenguaje de programación preferido.\n","Cargue el documento que desea clasificar en la biblioteca de clientes.\n","Llame a la API de Natural Language para clasificar el documento.\n","La API devolverá una lista de categorías posibles para el documento, junto con sus respectivas puntuaciones de confianza.\n","\n","## Amazon Comprehend\n","\n","Para utilizar Amazon Comprehend para la clasificación de documentos, siga estos pasos:\n","\n","Cree una cuenta de Amazon Web Services (AWS) y habilite el servicio Comprehend.\n","Instale la biblioteca de clientes de Comprehend para su lenguaje de programación preferido.\n","Cargue el documento que desea clasificar en la biblioteca de clientes.\n","Llame a la API de Comprehend para clasificar el documento.\n","La API devolverá una lista de categorías posibles para el documento, junto con sus respectivas puntuaciones de confianza.\n","Azure Text Analytics\n","\n","Para utilizar Azure Text Analytics para la clasificación de documentos:\n","\n","Cree una cuenta de Microsoft Azure y habilite el servicio Text Analytics.\n","Instale la biblioteca de clientes de Text Analytics para su lenguaje de programación preferido.\n","Cargue el documento que desea clasificar en la biblioteca de clientes.\n","Llame a la API de Text Analytics para clasificar el documento.\n","La API devolverá una lista de categorías posibles para el documento, junto con sus respectivas puntuaciones de confianza.\n","IBM Watson Language Understanding\n","\n","Para utilizar IBM Watson Language Understanding para la clasificación de documentos:\n","\n","Cree una cuenta de IBM Cloud y habilite el servicio Watson Language Understanding.\n","Instale la biblioteca de clientes de Watson Language Understanding para su lenguaje de programación preferido.\n","Cargue el documento que desea clasificar en la biblioteca de clientes.\n","Llame a la API de Watson Language Understanding para clasificar el documento.\n","La API devolverá una lista de categorías posibles para el documento, junto con sus respectivas puntuaciones de confianza.\n","\n","## SpaCy\n","\n","Para utilizar SpaCy para la clasificación de documentos:\n","\n","Instale SpaCy y el paquete de modelos de clasificación de texto.\n","Cargue el documento que desea clasificar en SpaCy.\n","Utilice SpaCy para extraer las características del documento.\n","Entrene un modelo de clasificación de texto con las características extraídas.\n","Utilice el modelo entrenado para clasificar el documento."],"metadata":{"id":"muMWNf56w1Ka"}},{"cell_type":"markdown","source":["Google Cloud Natural Language API\n","Google Cloud Natural Language API cobra $0.0003 por documento para la clasificación. Para 1000 tweets, el costo sería de $0.30.\n","\n","Amazon Comprehend\n","Amazon Comprehend cobra $0.006 por texto para la clasificación. Para 1000 tweets, el costo sería de $6.00.\n","\n","Azure Text Analytics\n","Azure Text Analytics cobra $0.0045 por texto para la clasificación. Para 1000 tweets, el costo sería de $4.50.\n","\n","IBM Watson Language Understanding\n","IBM Watson Language Understanding cobra $0.0025 por texto para la clasificación. Para 1000 tweets, el costo sería de $2.50.\n","\n","SpaCy\n","SpaCy es una biblioteca de código abierto, por lo que no hay un costo directo por usarla para la clasificación. Sin embargo, puede haber costos indirectos, como el costo de alojar y mantener un servidor para ejecutar SpaCy.\n","\n","Tenga en cuenta que estos son solo estimados y el costo real puede variar según sus patrones de uso y el nivel de precios que elija.\n","\n"],"metadata":{"id":"EVensos23UWM"}},{"cell_type":"markdown","source":["\n","Google Cloud\n","```\n","import io\n","from google.cloud import language\n","\n","# Crea una instancia del cliente de Natural Language\n","client = language.LanguageServiceClient()\n","\n","# Carga el documento que desea clasificar\n","with io.open(\"documento.txt\", \"r\", encoding=\"utf-8\") as f:\n","    document = client.document_from_text(content=f.read())\n","\n","# Clasifica el documento\n","classification = client.classify_document(document=document)\n","\n","# Imprime la categoría más probable para el documento\n","print(classification.categories[0].name)\n","\n","```\n","\n"],"metadata":{"id":"drFVdyDuxEoC"}},{"cell_type":"markdown","source":["Amazon Comprehend\n","\n","```\n","import boto3\n","\n","# Crea una instancia del cliente de Comprehend\n","comprehend = boto3.client(\"comprehend\")\n","\n","# Carga el documento que desea clasificar\n","with open(\"documento.txt\", \"rb\") as f:\n","    document = f.read()\n","\n","# Clasifica el documento\n","response = comprehend.detect_dominant_language(Text=document)\n","\n","# Imprime el idioma más probable para el documento\n","print(response[\"Languages\"][0][\"LanguageCode\"])\n","```\n","\n"],"metadata":{"id":"88JJkBIrxIFm"}},{"cell_type":"markdown","source":["Azurre\n","\n","```\n","import azure.cognitiveservices.textanalytics as textanalytics\n","\n","# Crea una instancia del cliente de Text Analytics\n","client = textanalytics.TextAnalyticsClient(credentials=credentials)\n","\n","# Carga el documento que desea clasificar\n","with open(\"documento.txt\", \"rb\") as f:\n","    document = f.read()\n","\n","# Clasifica el documento\n","response = client.classify_text(documents=[document])\n","\n","# Imprime la categoría más probable para el documento\n","print(response.classifications[0].categories[0].label)\n","```\n","\n"],"metadata":{"id":"Wpr4r2_ZxM66"}},{"cell_type":"code","source":[],"metadata":{"id":"adsndh8NyfwZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","# Carga el conjunto de datos de entrenamiento\n","df_train = pd.read_csv(\"train.csv\", encoding=\"utf-8\")\n","\n","# Prepara los datos de entrenamiento\n","X_train = df_train[\"texto\"]\n","y_train = df_train[\"categoria\"]\n","\n","# Crea una pipeline\n","\n","\n","# vectorizer: Transforma el texto en un vector de recuento de palabras.\n","\n","# tfidf: Transforma el vector de recuento de palabras en un vector TF-IDF.\n","\n","# clf: Clasifica el texto utilizando un modelo de regresión logística.\n","\n","pipeline = Pipeline([\n","    (\"vectorizer\", CountVectorizer()),\n","    (\"tfidf\", TfidfTransformer()),\n","    (\"clf\", LogisticRegression()),\n","])\n","\n","# Define el espacio de parámetros para GridSearchCV\n","\n","\n","# clf__C: Hiperparámetro que controla la regularización L1 del modelo.\n","\n","# clf__penalty: Hiperparámetro que especifica el tipo de regularización (L1 o L2).\n","\n","param_grid = {\n","    \"clf__C\": [0.01, 0.1, 1.0, 10.0],\n","    \"clf__penalty\": [\"l1\", \"l2\"],\n","}\n","\n","# Crea un objeto GridSearchCV\n","\n","\n","# cv: Número de pliegues para la validación cruzada.\n","\n","gs_clf = GridSearchCV(pipeline, param_grid, cv=5)\n","\n","# Entrena el modelo\n","\n","\n","# gs_clf.fit() entrena el modelo utilizando una validación cruzada de 5 pliegues.\n","\n","gs_clf.fit(X_train, y_train)\n","\n","# Imprime el mejor modelo\n","\n","\n","# gs_clf.best_estimator_ devuelve el mejor modelo según la validación cruzada.\n","\n","print(gs_clf.best_estimator_)\n","```\n","\n"],"metadata":{"id":"GRJB35onydLF"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.svm import SVC\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","\n","# Carga el conjunto de datos de entrenamiento\n","df_train = pd.read_csv(\"train.csv\", encoding=\"utf-8\")\n","\n","# Prepara los datos de entrenamiento\n","X_train = df_train[\"texto\"]\n","y_train = df_train[\"categoria\"]\n","\n","# Crea una pipeline\n","\n","# vectorizer: Transforma el texto en un vector de recuento de palabras.\n","# tfidf: Transforma el vector de recuento de palabras en un vector TF-IDF.\n","# clf: Clasifica el texto utilizando una máquina de vectores de soporte (SVM).\n","\n","pipeline = Pipeline([\n","    (\"vectorizer\", CountVectorizer()),\n","    (\"tfidf\", TfidfTransformer()),\n","    (\"clf\", SVC()),\n","])\n","\n","# Define el espacio de parámetros para GridSearchCV\n","\n","# clf__C: Hiperparámetro que controla la regularización L1 del modelo.\n","# clf__kernel: Hiperparámetro que especifica el tipo de núcleo de la SVM.\n","\n","param_grid = {\n","    \"clf__C\": [0.01, 0.1, 1.0, 10.0],\n","    \"clf__kernel\": [\"linear\", \"rbf\"],\n","}\n","\n","# Crea un objeto GridSearchCV\n","\n","# cv: Número de pliegues para la validación cruzada.\n","\n","gs_clf = GridSearchCV(pipeline, param_grid, cv=5)\n","\n","# Entrena el modelo\n","\n","# gs_clf.fit() entrena el modelo utilizando una validación cruzada de 5 pliegues.\n","\n","gs_clf.fit(X_train, y_train)\n","\n","# Imprime el mejor modelo\n","\n","# gs_clf.best_estimator_ devuelve el mejor modelo según la validación cruzada.\n","\n","print(gs_clf.best_estimator_)"],"metadata":{"id":"3qF95ZER0dFD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El núcleo RBF es una buena elección para muchas tareas de aprendizaje automático, como la clasificación, la regresión y la agrupación. Es especialmente adecuado para tareas en las que los datos están separados de forma no lineal."],"metadata":{"id":"GExhnwpH1nE-"}},{"cell_type":"markdown","source":["## USANDO OPEN IA\n","\n","\n","\n","```\n","!pip uninstall openai\n","!pip install openai\n","```\n","\n","\n","Esta conexión es para OpenAI\n","```\n","import openai\n","openai.api_key = 'ESTA LLAVE CUESTA'\n","```\n","\n","\n","\n","```\n","pais='Chile'\n","directorio=\"\"\n","df_2 = pd.read_csv(directorio+'cleaned_tweets_parties_'+pais+'_g.csv')\n","df_1 = pd.read_csv(directorio+'Chile/data_problemas_Chile.csv')\n","```\n","\n","\n","\n","```\n","prompt = (\n","    \"Please classify the content of tweets from Chilean congressmen. Assign a number from the \"\n","    \"following list of topics based on the central theme or issue of the tweet. If the tweet's content \"\n","    \"does not align with any of these topics, assign a 0. The topics, along with some indicative keywords, are:\\n\"\n","    \"1. Inflación (Inflation) - Keywords: precios, costo de vida, aumento, economía, alza de precios, incremento, subida, devaluacion, alza de canasta basica\\n\"\n","    \"2. Inseguridad Ciudadana y delincuencia (Public insecurity and crime) - Keywords: seguridad, delitos, policía, crimen, desconfianza, delincuencia, vulnerabilidad, temor\\n\"\n","    \"3. Salud/Seguridad Social (Health/Social Security) - Keywords: hospitales, médicos, pensiones, salud pública, jubilaciones, covid, virus, vacunas, bienestar, paciente, essalud\\n\"\n","    \"4. Educación (Education) - Keywords: escuelas, universidades, estudiantes, reforma educativa, alfabetizacion, profesor, enseñanza, aprendizaje\\n\"\n","    \"5. Derechos de los grupos étnicos y culturales (Rights of ethnic and cultural groups) - Keywords: indígenas, cultura, lengua, diversidad, minorias, discriminacion, pluricultural, identidad\\n\"\n","    \"6. Desempleo y Subempleo (Unemployment and Underemployment) - Keywords: trabajo, empleo, economía laboral, oportunidades, tasa de desempleo, crisis laboral, subempleo, informal\\n\"\n","    \"7. Medio Ambiente (Environment) - Keywords: naturaleza, contaminación, protección ambiental, cambio climático, arboles, aves, incendios, sostenibilidad, biodiversidad, conservacion\\n\"\n","    \"8. Conflictos entre los poderes del Estado (Conflicts between the powers of the State) - Keywords: congreso, gobierno, ley, constitución, separacion de poder, institucional, autonomia, legislativo, ejecutivo, judicial\\n\"\n","    \"9. Narcotráfico (Drug trafficking) - Keywords: drogas, narcóticos, fronteras, policía, narcotrafico, pandillas, delincuencia organizada, cocaína\\n\"\n","    \"Analyze the tweets provided below, and for each, indicate only the topic number(s) it pertains to (NEVER A TEXT), based on the central theme of the tweet in relation to the topics and keywords listed. If the tweet is unrelated to these topics, or if you're unable to determine the topic due to lack of context or clarity, assign a 0.\"\n",")\n","\n","```\n","\n","\n","\n","\n"],"metadata":{"id":"QQR9p4nhzNu0"}},{"cell_type":"code","source":[],"metadata":{"id":"QyD6qcLF5EGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Pb0tmCtH47jf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QUVAb5a2430G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mZw066WI42h_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQDX4R764Rc6"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"W3RaiW1D4XSZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TTSmJW1L4XWD"},"execution_count":null,"outputs":[]}]}